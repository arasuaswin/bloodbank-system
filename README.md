# Cloud-Native BloodBank Management System

A production-ready, highly available blood bank management system built with **Next.js**, **Prisma**, and **MySQL**. Engineered with a strong focus on advanced **AWS Cloud Architecture**, **DevSecOps**, and fully automated **CI/CD pipelines**.

## Tech Stack

- **Frontend:** Next.js 16, React, TypeScript, Tailwind CSS, shadcn/ui, Framer Motion
- **Backend:** Next.js API Routes, Prisma ORM
- **Database:** MySQL
- **Authentication:** NextAuth.js v5 (Credentials provider)
- **Email:** AWS SES
- **Deployment:** Docker, Terraform (AWS)

## Features

### Public
- Blood availability search with real-time stock display
- Blood request submission
- Donor registration (Immediate signup)
- Recipient registration

### Admin Portal
- Dashboard with key statistics and blood stock chart
- Donor management (view, delete)
- Blood stock management (add, subtract, set quantities)
- Blood request approval/rejection (with stock deduction)
- Appointment management (approve, reject, mark complete)
- Recipient management

### Donor Portal
- Personal dashboard with eligibility and donation stats
- Appointment booking and history tracking
- Profile management (contact info, health details)

---

## ğŸ’» Local Development Setup

### Prerequisites
- Node.js 20+
- MySQL database

### Setup Steps
1. Clone the repository and navigate to the Next.js directory:
   ```bash
   git clone https://github.com/arasuaswin/bloodbank-system.git
   cd bloodbank-system/bloodbank-next
   ```
2. Copy the environment file and configure your database and secrets:
   ```bash
   cp .env.example .env
   ```
3. Install dependencies:
   ```bash
   npm install
   ```
4. Push the database schema and seed default data:
   ```bash
   npx prisma db push
   node prisma/seed.js
   ```
6. Start the development server:
   ```bash
   npm run dev
   ```

### Local Production Build (Windows)
If you want to test the fully compiled production version of the application locally, you can use the included `deploy.bat` script inside the Next.js directory:

```cmd
cd bloodbank-next
deploy.bat
```
This script will automatically generate the Prisma client, push your database schema, run the Next.js production compiler, and start the application on `http://localhost:3000`.

---

## â˜ï¸ AWS Architecture & Deployment Guide

This project includes production-ready **Terraform Configuration** to deploy the entire stack to AWS in minutes. 

### AWS Architecture Overview

```mermaid
graph TD
    %% Define Styles
    classDef aws fill:#FF9900,stroke:#232F3E,stroke-width:2px,color:white;
    classDef vpc fill:#F3F3F3,stroke:#3F8624,stroke-width:2px,stroke-dasharray: 5 5;
    classDef public fill:#E6F0FA,stroke:#0052CC,stroke-width:1px;
    classDef private fill:#FAD7E1,stroke:#E0004D,stroke-width:1px;
    classDef cicd fill:#F9F9F9,stroke:#5A6B86,stroke-width:2px,stroke-dasharray: 5 5;

    User([ğŸ‘¤ End User]) -->|HTTPS| CF[ğŸŒ AWS CloudFront]
    
    %% Main VPC Architecture
    subgraph VPC ["â˜ï¸ AWS Virtual Private Cloud (VPC)"]
        direction TB
        IGW[ğŸšª Internet Gateway]
        
        %% Public Tier
        subgraph PublicSubnets ["ğŸŒ Public Subnets (AZ-a / AZ-b)"]
            ALB[âš–ï¸ Application Load Balancer]
            NAT[ğŸ”„ NAT Gateway]
            ECS[ğŸ³ Amazon ECS<br>Fargate & Spot Instances]
        end
        
        %% Private Tier
        subgraph PrivateSubnets ["ğŸ”’ Private Subnets (AZ-a / AZ-b)"]
            RDS[(ğŸ—„ï¸ Amazon RDS MySQL 8.0)]
        end
        
        %% Network flow
        IGW --- PublicSubnets
        ALB -->|Port 3000| ECS
        ECS -->|Outbound Web/Updates| NAT
        NAT --> IGW
        ECS -->|Port 3306| RDS
    end

    %% External Connections from VPC
    CF -->|HTTP/HTTPS| ALB
    ECS -->|API via NAT| SES[ğŸ“§ Amazon SES]
    
    %% CI/CD Setup
    subgraph CICD ["âš™ï¸ CI/CD & Security"]
        direction LR
        GH[(ğŸ™ GitHub Repo)] -.->|Webhook| CP[ğŸ›£ï¸ AWS CodePipeline]
        CP -.-> CB[ğŸ› ï¸ AWS CodeBuild]
        CB -.->|Builds & Pushes| ECR[ğŸ“¦ Amazon ECR]
        CB -.->|Triggers Update| ECS
        
        SM[ğŸ”‘ AWS Secrets Manager] -.->|Injects DB/Auth Secrets| ECS
        KMS[ğŸ—ï¸ AWS KMS] -.->|Encrypts| RDS
        KMS -.->|Encrypts| ECR
    end
    
    %% Apply Styles
    class CF,ALB,ECS,RDS,NAT,IGW,SES,CP,CB,ECR,SM,KMS aws;
    class VPC vpc;
    class PublicSubnets public;
    class PrivateSubnets private;
    class CICD cicd;
```

### Global Compliance & Infrastructure Standards
This system was engineered using the **AWS Well-Architected Framework**, inherently complying with strict international data protection standards (such as **HIPAA**, **GDPR**, and **SOC 2**). 

#### 1. Deployment Region & High Availability
* **Primary Region:** The infrastructure defaults to the **Mumbai Region (`ap-south-1`)**. 
* **Multi-AZ Fault Tolerance:** The network is intentionally divided across two physically isolated Availability Zones (**`ap-south-1a`** and **`ap-south-1b`**). This ensures that if a massive power failure or flood takes down one AWS data center, the Application Load Balancer automatically shifts all traffic to backend Fargate containers and database replicas running safely in the second data center.

#### 2. International Security Guidelines Followed
* **The "Zero-Trust & Air-Gapped" Standard:** International healthcare and privacy standards dictate that sensitive databases must never touch the public internet. The `rds.tf` expressly places the MySQL database into a strict **Private Subnet** without a public IP. Only internal backend Fargate containers inside the VPC are allowed to communicate with it.
* **Encryption "In Transit" and "At Rest" (SOC 2):** 
  * **In Transit:** The `alb.tf` and `cloudfront.tf` forcibly redirect all HTTP traffic to HTTPS, encrypting data between the user's browser and AWS servers.
  * **At Rest:** The `kms.tf` (Key Management Service) provisions a cryptographic key attached directly to the database. Hard drives are physically encrypted before they are written to by AWS.
* **The "Principle of Least Privilege" (NIST):** Containers should only have exactly the permissions they need. The Docker container executes Next.js as an **unprivileged user (uid 1001)** instead of `root`, preventing container-escape hacks. AWS IAM Roles restrict the Fargate servers to discrete functions (like sending SES emails) rather than granting broad administrative access.
* **Zero Hardcoded Secrets Policy:** Major compliance standards strictly forbid storing plaintext passwords in GitHub repositories. Terraform uses `aws_secretsmanager_secret` to store the database and auth secrets in an encrypted vault, securely injecting them into the Next.js container's memory only at the exact millisecond it boots.

### CI/CD Practices (Continuous Integration & Deployment)
* **Fully Automated Pipeline**: Built using AWS CodePipeline. Pushing changes to the `main` GitHub branch automatically triggers a build.
* **Build Phase**: AWS CodeBuild creates a Docker container, compiles the Next.js app, and pushes the optimized image to Amazon ECR.
* **Zero-Downtime Rolling Updates**: AWS ECS gracefully rolls out new container instances and drains old ones only when the new instances pass ALB health checks.
* **Circuit Breaker Rollbacks**: If a bad deployment occurs (e.g., application crashes on start), ECS automatically halts the deployment and rolls back to the previous stable version.
* **Secure Secret Injection**: No passwords are kept in code or environment files on the server. AWS Secrets Manager injects credentials securely into the ECS task at runtime.

### Containerization & Application Performance
This project has been heavily optimized for cloud environments, significantly reducing AWS networking costs and startup latency:
1. **Next.js Standalone Output**: The Next.js configuration (`output: "standalone"`) automatically traces imports and bundles only the necessary node_modules. This shrinks the Docker image size by over **80%**, drastically reducing ECR storage costs and accelerating ECS deployment times.
2. **Multi-Stage Docker Builds**: The `Dockerfile` separates dependency installation (`deps`), compilation (`builder`), and execution (`runner`). The final production image discards the heavy compilation toolchains entirely.
3. **Least Privilege Execution**: The Docker container refuses to run as root. It creates a dedicated unprivileged `nextjs` user (`uid 1001`), completely nullifying entire classes of container escape vulnerabilities.
4. **Auto-Migration Script**: Database schema pushing is not an afterthought. The container spins up executing `start.js`, which detects if the Prisma database is in sync with the application code and automatically triggers migrations *before* booting the listener.

### AWS Services & Cost Optimization Strategy
This project leverages a vast array of enterprise-grade AWS services. Architectural decisions were made to provide 99.99% high availability while keeping the billing tightly optimized:

#### Estimated Monthly Infrastructure Costs
By strictly utilizing Spot instances, a single shared NAT Gateway, and micro-databases, this enterprise-grade architecture has been aggressively optimized down to approximately **~$65.00 - $75.00 / month**.

| Service | Specification / Optimization | Est. Monthly Cost |
|---------|------------------------------|-------------------|
| **Single NAT Gateway** | 1 NAT Gateway (Shared across all Subnets) | ~$32.00 |
| **Application Load Balancer** | 1 ALB + minimal LCU usage | ~$16.00 |
| **Amazon RDS (MySQL)** | `db.t3.micro` + 20GB `gp3` storage | ~$13.00 |
| **ECS Fargate Spot** | 1-4 Tasks (0.25 vCPU, 0.5GB RAM) on Spot | ~$3.00 - $5.00 |
| **CloudFront, KMS, SES, Secrets Manager** | Pay-per-use (Highly caches / Free Tier applicable) | ~$1.00 - $4.00 |
| **Total** | | **~$65.00 - $70.00 / month** |

#### 1. Compute & Container Orchestration
* **Amazon ECS & Fargate Spot:** Orchestrates the Next.js Docker containers serverlessly. Auto-scaling is configured to dynamically scale from 1 to 4 instances when CPU utilization crosses 70%.
  * **Cost Optimization:** Instead of standard on-demand Fargate pricing, the cluster is assigned a `FARGATE_SPOT` capacity provider weight. This runs the application on surplus AWS compute power, yielding up to **70% cost savings**.

#### 2. Global Delivery & Network Security
* **Amazon CloudFront:** A global CDN that distributes Next.js static assets (`/_next/static/*` and `/images/*`) and caches them at edge locations worldwide. It enforces HTTPS and acts as a shield against DDoS attacks.
  * **Cost Optimization:** By offloading static file serving to CloudFront, we reduce the traffic and CPU load on the ECS containers. We explicitly restrict the `PriceClass` to `PriceClass_200` to avoid the most expensive edge regions (like South America).
* **Application Load Balancer (ALB):** Terminates SSL connections and distributes internet traffic evenly among the Fargate containers across different Availability Zones. 
  * **Cost Optimization & Impact:** This enables **Zero-Downtime Deployments**. The Target Group is finely tuned to health-check new containers and deregister failing ones extremely quickly, ensuring no traffic or compute seconds are wasted on crashed instances.
* **Amazon VPC (Multi-AZ Architecture) & Single NAT Gateway:** The network is intentionally divided across **Two separate Availability Zones (AZ-a and AZ-b)**. It features 2 Public Subnets (for the ALB) and 2 Private Subnets (for Fargate and RDS) spanning distinct physical data centers. This ensures the application remains online even if an entire AWS facility goes offline.
  * **Cost Optimization:** Enterprise setups often use one NAT Gateway *per Availability Zone* for perfect redundancy, which doubles or triples the massive hourly network fee. To balance cost and availability, we explicitly provision a **Single NAT Gateway** for the entire VPC, instantly slashing fixed monthly network pricing by 50% while still allowing private instances across both AZs to pull updates and send outbound SES API calls.

#### 3. Database & Storage
* **Amazon RDS MySQL 8.0:** A fully managed, highly secure relational database hidden safely inside the Private Subnets. It provides automated daily backups and multi-AZ failover capabilities if configured.
  * **Cost Optimization:** Provisions a `db.t3.micro` instance (highly affordable) paired with `gp3` auto-scaling storage. You do not pay for 100GB of heavy storage upfront; AWS only bills you for the literal megabytes you are currently using.
* **Amazon S3:** Used temporarily by the CodePipeline to store zipped source code artifacts during the build process.

#### 4. Automated CI/CD Setup
* **AWS CodePipeline, CodeBuild, & ECR:** The CI/CD workflow natively listens to the GitHub `main` branch. Pushing code triggers CodeBuild to spin up a temporary Linux server, execute the Docker build, and push the image to the **Elastic Container Registry (ECR)**. ECR is configured to automatically scan images for vulnerabilities on push. CodePipeline then triggers ECS to perform a rolling update natively.

#### 5. Cryptography & Security
* **AWS Secrets Manager:** A highly secure vault for sensitive strings (e.g., `DATABASE_URL`, `ADMIN_PASSWORD`, `NEXTAUTH_SECRET`). Instead of hardcoding passwords, ECS securely injects these secrets at the exact moment the Fargate container boots up.
* **AWS KMS:** Manages the cryptographic keys used to automatically enforce encryption-at-rest for both the RDS Database volumes and the ECR Docker images.
* **AWS IAM Roles:** Granular permissions are attached to Fargate tasks to ensure containers only have access to exactly what they need (e.g., SES sending permissions, Secrets Manager reading permissions) and nothing more.

#### 6. Monitoring & Automated Alerting
* **Amazon CloudWatch:** Streams all real-time Next.js `console.log` outputs directly into a `/ecs/bloodbank-gms` Log Group. Three custom **CloudWatch Alarms** are continuously monitoring:
  1. ECS CPU High (>80%)
  2. RDS CPU High (>80%)
  3. ALB Healthy Hosts Low (Containers crashed)
* **Amazon SNS (Simple Notification Service):** If any of the above CloudWatch Alarms trigger, SNS instantly blasts an emergency email alert to the administrator's inbox.
* **Amazon SES (Simple Email Service):** Allows the backend API to physically send verified outbound emails (OTP codes, appointments). By using the AWS SDK and IAM roles attached to the ECS task, the application naturally authenticates with SES without ever needing a hardcoded SMTP password.

### Step-by-Step AWS Setup for forkers

If you have forked this repository and want to host it on your own AWS account, follow these steps strictly:

#### 1. Setup Deployment Variables
All passwords and sensitive tokens are managed via Terraform variables and injected into AWS Secrets Manager. 

Navigate to the terraform directory:
```bash
cd terraform
```
Copy the example variables file:
```bash
cp terraform.tfvars.example terraform.tfvars
```
**Open `terraform.tfvars` and fill it out completely.** 
* Set strong passwords for `db_password` (min 12 chars), `admin_password` (min 8 chars), and `nextauth_secret` (generate one using `openssl rand -base64 32`).
* Provide your GitHub repository string (e.g., `arasuaswin/bloodbank-system`).

> âš ï¸ **Note:** `terraform.tfvars` is intentionally included in `.gitignore` so your secrets are never accidentally pushed to GitHub.

#### 2. Deploy Infrastructure
We have provided comprehensive PowerShell scripts that completely automate the provisioning, Docker building, database pushing, and AWS deployments natively for Windows machines. 

Open a PowerShell terminal as Administrator, authenticate with your AWS CLI locally, and run:

```powershell
.\deploy_aws.ps1
```
This single script will:
- Check for all prerequisite tools (Docker, AWS CLI, Terraform)
- Run `terraform init` and `terraform apply` for you
- Build the Next.js Docker image and push it to Amazon ECR
- Attempt to automate Prisma database schema pushing
- Force-deploy the latest container onto your ECS cluster

#### 3. Confirm AWS Email Verifications
As Terraform provisions your resources, AWS will automatically send two important emails to the `notification_email` you provided in `terraform.tfvars`:
1. **AWS Notification - Subscription Confirmation:** This is from Amazon SNS. Click the `Confirm subscription` link in this email to authorize AWS to send you emergency CloudWatch infrastructure alerts.
2. **Amazon Web Services â€“ Email Address Verification Request:** This is from Amazon SES. Click the verification link to securely prove you own the email address. This allows your backend API to send outbound emails (like appointments and OTPs) naturally using this address.

> âš ï¸ **Critical:** Your system cannot send or receive emails until you physically click both links!

#### 4. Connect GitHub to AWS CodePipeline
Because your AWS account needs permission to read your public GitHub repository during automated builds, Terraform creates a pending connection.

1. Log into your **AWS Console**.
2. Navigate to **Developer Tools** â¡ï¸ **Settings** â¡ï¸ **Connections**.
3. You will see a connection named `bloodbank-gms-github-connection` in a `Pending` state.
4. Click on it and choose **Update pending connection**.
5. Follow the prompts to authorize AWS to access your GitHub repositories.

#### 5. Future Deployments & CI/CD
Once the GitHub connection is active and your first Fargate instances are live:
* **To push a new update**: Simply commit your changes to the `main` branch of your GitHub repository. AWS CodePipeline will automatically fetch the new code, rebuild the Docker image, and perform a rolling update natively.

---

## ğŸ—‘ï¸ Tearing Down Infrastructure

If you wish to completely destroy the AWS infrastructure to stop incurring costs, simply run the included undeploy script from a PowerShell window:

```powershell
.\undeploy_aws.ps1
```
This script will empty the S3 artifact buckets and safely run `terraform destroy` to remove all AWS resources.

---

## Default Admin Credentials
- **Email:** admin@bloodbank.com
- **Password:** The password you defined as `admin_password` in `terraform.tfvars`

## License
This project is for educational purposes.
